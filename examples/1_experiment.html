

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>How to run an experiment &mdash; Pyrado 0.2 documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/graphviz.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="How to wrap an environment" href="2_environment_wrappers.html" />
    <link rel="prev" title="Pyrado’s Documentation" href="../index.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> Pyrado
          

          
          </a>

          
            
            
              <div class="version">
                0.2
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Examples:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">How to run an experiment</a></li>
<li class="toctree-l1"><a class="reference internal" href="2_environment_wrappers.html">How to wrap an environment</a></li>
<li class="toctree-l1"><a class="reference internal" href="3_algorithm.html">How to create an algorithm</a></li>
</ul>
<p class="caption"><span class="caption-text">Package Overview:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../algorithms.html">algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../domain_randomization.html">domain_randomization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../environments.html">environments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../environments.pysim.html">pysim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../environments.mujoco.html">mujoco</a></li>
<li class="toctree-l1"><a class="reference internal" href="../environments.rcspysim.html">rcspysim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../environments.quanser.html">quanser</a></li>
<li class="toctree-l1"><a class="reference internal" href="../environment_wrappers.html">environment_wrappers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exploration.html">exploration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../logger.html">logger</a></li>
<li class="toctree-l1"><a class="reference internal" href="../plotting.html">plotting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../policies.html">policies</a></li>
<li class="toctree-l1"><a class="reference internal" href="../sampling.html">sampling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../spaces.html">spaces</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tasks.html">tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils.html">utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tests.html">tests</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Pyrado</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>How to run an experiment</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/examples/1_experiment.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="how-to-run-an-experiment">
<h1>How to run an experiment<a class="headerlink" href="#how-to-run-an-experiment" title="Permalink to this headline">¶</a></h1>
<p>This file provides a step-by-step example of how to write a training script in Pyrado.
There are many valid possibilities to deviate from this scheme. However, the following sequence is battle-tested.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pyrado</span>
<span class="kn">from</span> <span class="nn">pyrado.algorithms.hc</span> <span class="kn">import</span> <span class="n">HCNormal</span>
<span class="kn">from</span> <span class="nn">pyrado.environment_wrappers.action_normalization</span> <span class="kn">import</span> <span class="n">ActNormWrapper</span>
<span class="kn">from</span> <span class="nn">pyrado.environments.pysim.ball_on_beam</span> <span class="kn">import</span> <span class="n">BallOnBeamSim</span>
<span class="kn">from</span> <span class="nn">pyrado.logger.experiment</span> <span class="kn">import</span> <span class="n">setup_experiment</span><span class="p">,</span> <span class="n">save_list_of_dicts_to_yaml</span>
<span class="kn">from</span> <span class="nn">pyrado.policies.features</span> <span class="kn">import</span> <span class="n">FeatureStack</span><span class="p">,</span> <span class="n">identity_feat</span><span class="p">,</span> <span class="n">sin_feat</span>
<span class="kn">from</span> <span class="nn">pyrado.policies.linear</span> <span class="kn">import</span> <span class="n">LinearPolicy</span>
<span class="kn">from</span> <span class="nn">pyrado.sampling.rollout</span> <span class="kn">import</span> <span class="n">rollout</span><span class="p">,</span> <span class="n">after_rollout_query</span>
<span class="kn">from</span> <span class="nn">pyrado.utils.data_types</span> <span class="kn">import</span> <span class="n">RenderMode</span>
<span class="kn">from</span> <span class="nn">pyrado.utils.input_output</span> <span class="kn">import</span> <span class="n">print_cbt</span>
</pre></div>
</div>
<p>Start by creating a new experiment which’s folder will be placed in <cite>Pyrado/data/temp</cite> by default. You can change this
by passing any directory as <cite>base_dir</cite>. In Pyrado, the folders are structured like this:
<cite>environment_name/algorithm_name/datetime_and_info</cite>. This rule is only required for the automatic search for experiments
(e.g. used in <cite>sim_policy()</cite>). This search function requires the individual experiment folders to start with <cite>date_time</cite>.
Aside from this, you can name your experiments and folders however you like. Use the <cite>load_experiment()</cite> function to
later oad your results. It will look for an environment as well as a policy file in the provided path.
Additionally, <cite>setup_experiment()</cite> can set a seed for the random number generators (see <cite>set_seed()</cite>. It is highly
suggested to do so, if you want to compare changes of certain hyper-parameters to eliminate the effect of the initial
state and the initial policy parameters (both are sampled randomly in most cases).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ex_dir</span> <span class="o">=</span> <span class="n">setup_experiment</span><span class="p">(</span><span class="n">BallOnBeamSim</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">HCNormal</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s1">_</span><span class="si">{</span><span class="n">LinearPolicy</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="s1">&#39;ident-sin&#39;</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">1001</span><span class="p">)</span>
</pre></div>
</div>
<p>Set up the environment a.k.a. domain to train in. After creating the environment, you can apply various wrappers which
are modular. Note that the order of wrappers might be of importance. For example, wrapping an environment with an
<cite>ObsNormWrapper</cite> and then with an <cite>GaussianObsNoiseWrapper</cite> applies the noise on the normalized observations, and yields
different results than the reverse order of wrapping.
Environments in Pyrado can be of different types: (i) written in Python only (like the Qunaser simulations or simple
OpenAI Gym environments), (ii) wrapped as well as self-designed MuJoCo-based simulations, or (iii) self-designed
robotic environments powered by Rcs using either the Bullet or Vortex physics engine. None of the simulations includes
any computer vision aspects. It is all about dynamics-based interaction and (continuous) control. The degree of
randomization for the environments varies strongly, since it is a lot of work to randomize them properly (including
testing) and I have to graduate after all ;)</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">env_hparams</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
    <span class="n">dt</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="mf">50.</span><span class="p">,</span>
    <span class="n">max_steps</span><span class="o">=</span><span class="mi">300</span>
<span class="p">)</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">BallOnBeamSim</span><span class="p">(</span><span class="o">**</span><span class="n">env_hparams</span><span class="p">)</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">ActNormWrapper</span><span class="p">(</span><span class="n">env</span><span class="p">)</span>
</pre></div>
</div>
<p>Set up the policy after the environment since it needs to know the dimensions of the policies observation and action
space. There are many different policy architectures available under <cite>Pyrado/pyrado/policies</cite>, which significantly
vary in terms of required hyper-parameters. You can find some examples at <cite>Pyrado/scripts/training</cite>.
Note that all policies must inherit from <cite>Policy</cite> which inherits from <cite>torch.nn.Module</cite>. Moreover, all <cite>Policy</cite>
instances are deterministic. The exploration is handled separately (see <cite>Pyrado/pyrado/exploration</cite>).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">policy_hparam</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
    <span class="n">feats</span><span class="o">=</span><span class="n">FeatureStack</span><span class="p">([</span><span class="n">identity_feat</span><span class="p">,</span> <span class="n">sin_feat</span><span class="p">])</span>
<span class="p">)</span>
<span class="n">policy</span> <span class="o">=</span> <span class="n">LinearPolicy</span><span class="p">(</span><span class="n">spec</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">spec</span><span class="p">,</span> <span class="o">**</span><span class="n">policy_hparam</span><span class="p">)</span>
</pre></div>
</div>
<p>Specify the algorithm you want to use for learning the policy parameters.
For deterministic sampling, you need to set <cite>num_workers=1</cite>. If <cite>num_workers&gt;1</cite>, PyTorch’s multiprocessing
library will be used to parallelize sampling from the environment on the CPU. The resulting behavior is non-deterministic,
i.e. even for the same random seed, you will get different results.
The algorithms can be categorized in two different types: one type randomizes the action every step (their exploration
strategy inherits from <cite>StochasticActionExplStrat</cite>), and the other type randomizes the policy parameters once every
rollout their exploration strategy inherits from <cite>StochasticParamExplStrat</cite>). It goes without saying that every
algorithm has different hyper-parameters. However, they all use the same <cite>rollout()</cite> function to generate their data.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">algo_hparam</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
    <span class="n">max_iter</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">pop_size</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
    <span class="n">num_rollouts</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">expl_factor</span><span class="o">=</span><span class="mf">1.1</span><span class="p">,</span>
    <span class="n">expl_std_init</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span>
    <span class="n">num_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">algo</span> <span class="o">=</span> <span class="n">HCNormal</span><span class="p">(</span><span class="n">ex_dir</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">policy</span><span class="p">,</span> <span class="o">**</span><span class="n">algo_hparam</span><span class="p">)</span>
</pre></div>
</div>
<p>Save the hyper-parameters before staring the training in a YAML-file. This step is not strictly necessary, but it helps
you to later see which hyper-parameters you used, i.e. which setting leads to a successfully trained policy.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">save_list_of_dicts_to_yaml</span><span class="p">([</span>
    <span class="nb">dict</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">env_hparams</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">ex_dir</span><span class="o">.</span><span class="n">seed</span><span class="p">),</span>
    <span class="nb">dict</span><span class="p">(</span><span class="n">policy</span><span class="o">=</span><span class="n">policy_hparam</span><span class="p">),</span>
    <span class="nb">dict</span><span class="p">(</span><span class="n">algo</span><span class="o">=</span><span class="n">algo_hparam</span><span class="p">,</span> <span class="n">algo_name</span><span class="o">=</span><span class="n">algo</span><span class="o">.</span><span class="n">name</span><span class="p">)],</span>
    <span class="n">ex_dir</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Finally, start the training. The <cite>train()</cite> function is the same for all algorithms inheriting from the <cite>Algorithm</cite>
base class. It repetitively calls the algorithm’s custom <cite>step()</cite> and <cite>update()</cite> functions.
You can specify a <cite>load_dir</cite> to load and continue a previous experiment. This uses the <cite>load_snapshot()</cite> which should
be implemented for every algorithm. The <cite>snapshot_mode()</cite> determines when to save the current training state, e.g.
‘latest’ saves after every step of the algorithm, and ‘best’ only saves if the average return is a new highscore.
Moreover, you can set the random number generator’s seed. This second option for setting the seed comes in handy when
you want to continue from a previous experiment multiple times.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">algo</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">load_dir</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">snapshot_mode</span><span class="o">=</span><span class="s1">&#39;latest&#39;</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

<span class="nb">input</span><span class="p">(</span><span class="s1">&#39;Finished training. Hit any key to simulate the policy.&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Simulate the learned policy in the environment it has been trained in. The following is a part of
<cite>scripts/sim_policy.py</cite> which can be executed to simulate any policy given the experiment’s directory.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">done</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">param</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
<span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
    <span class="n">ro</span> <span class="o">=</span> <span class="n">rollout</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">policy</span><span class="p">,</span> <span class="n">render_mode</span><span class="o">=</span><span class="n">RenderMode</span><span class="p">(</span><span class="n">video</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="nb">eval</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">reset_kwargs</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">domain_param</span><span class="o">=</span><span class="n">param</span><span class="p">,</span> <span class="n">init_state</span><span class="o">=</span><span class="n">state</span><span class="p">))</span>
    <span class="n">print_cbt</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Return: </span><span class="si">{</span><span class="n">ro</span><span class="o">.</span><span class="n">undiscounted_return</span><span class="p">()</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">bright</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">done</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">param</span> <span class="o">=</span> <span class="n">after_rollout_query</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">policy</span><span class="p">,</span> <span class="n">ro</span><span class="p">)</span>
<span class="n">pyrado</span><span class="o">.</span><span class="n">close_vpython</span><span class="p">()</span>
</pre></div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="2_environment_wrappers.html" class="btn btn-neutral float-right" title="How to wrap an environment" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="../index.html" class="btn btn-neutral float-left" title="Pyrado’s Documentation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2020

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>